{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5976e658",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys, glob, uuid, time, itertools\n",
    "from typing import Iterable, Tuple, List, Dict, Generator\n",
    "from dotenv import load_dotenv\n",
    "from pypdf import PdfReader\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "\n",
    "# ----------------- Config (defaults; can be overridden via CLI/env) -----------------\n",
    "CHUNK_TOKENS_DEFAULT = 700\n",
    "OVERLAP_TOKENS_DEFAULT = 100\n",
    "EMBED_BATCH = 64          # texts per OpenAI embeddings request\n",
    "UPSERT_BATCH = 200       \n",
    "\n",
    "load_dotenv()  # load .env file if present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98ca2f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    " # vectors per Pinecone gRPC upsert\n",
    "\n",
    "# ----------------- Helpers -----------------\n",
    "def chunks(iterable: Iterable, batch_size: int) -> Iterable[tuple]:\n",
    "    it = iter(iterable)\n",
    "    batch = tuple(itertools.islice(it, batch_size))\n",
    "    while batch:\n",
    "        yield batch\n",
    "        batch = tuple(itertools.islice(it, batch_size))\n",
    "\n",
    "def read_pdf_pages(pdf_path: str) -> List[Tuple[int, str]]:\n",
    "    try:\n",
    "        r = PdfReader(pdf_path)\n",
    "    except Exception as e:\n",
    "        print(f\"!! Failed to open {pdf_path}: {e}\", file=sys.stderr)\n",
    "        return []\n",
    "    out = []\n",
    "    for i, p in enumerate(r.pages):\n",
    "        txt = (p.extract_text() or \"\").strip()\n",
    "        if txt:\n",
    "            out.append((i + 1, \" \".join(txt.split())))\n",
    "    return out\n",
    "\n",
    "def token_chunks(text: str, enc, chunk_tokens: int, overlap: int) -> List[str]:\n",
    "    ids = enc.encode(text)\n",
    "    out, start, n = [], 0, len(ids)\n",
    "    while start < n:\n",
    "        end = min(start + chunk_tokens, n)\n",
    "        out.append(enc.decode(ids[start:end]))\n",
    "        if end == n: break\n",
    "        start = max(0, end - overlap)\n",
    "    return out\n",
    "\n",
    "def iter_pdf_chunks(pdf_path: str, chunk_tokens: int, overlap: int) -> Generator[Tuple[str,int,int,str], None, None]:\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    for page, text in read_pdf_pages(pdf_path):\n",
    "        for ci, ch in enumerate(token_chunks(text, enc, chunk_tokens, overlap)):\n",
    "            yield pdf_path, page, ci, ch\n",
    "\n",
    "def embed_texts_openai(client: OpenAI, model: str, texts: List[str]) -> List[List[float]]:\n",
    "    # Basic retry for transient 5xx/429\n",
    "    backoff = 1.0\n",
    "    while True:\n",
    "        try:\n",
    "            resp = client.embeddings.create(model=model, input=texts)\n",
    "            return [d.embedding for d in resp.data]\n",
    "        except Exception as e:\n",
    "            msg = str(e).lower()\n",
    "            if any(x in msg for x in (\"429\", \"temporar\", \"timeout\", \"overload\", \"503\", \"500\")):\n",
    "                time.sleep(backoff)\n",
    "                backoff = min(backoff * 2, 30)\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "# ----------------- Main ingest -----------------\n",
    "def ingest_folder(folder: str,\n",
    "                  index_host: str,\n",
    "                  namespace: str,\n",
    "                  embed_model: str,\n",
    "                  chunk_tokens: int,\n",
    "                  overlap_tokens: int):\n",
    "    # Gather PDFs\n",
    "    pdfs = sorted(glob.glob(os.path.join(folder, \"**/*.pdf\"), recursive=True))\n",
    "    if not pdfs:\n",
    "        print(f\"No PDFs found under: {folder}\")\n",
    "        return\n",
    "\n",
    "    # Init clients\n",
    "    pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "    index = pc.Index(host=index_host)   # gRPC connects by host\n",
    "    oai = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "    total_files = 0\n",
    "    total_chunks = 0\n",
    "\n",
    "    def gen_upserts() -> Iterable[Dict]:\n",
    "        nonlocal total_chunks\n",
    "        for pdf_path in pdfs:\n",
    "            rel_name = os.path.relpath(pdf_path, folder)\n",
    "            file_id_prefix = os.path.basename(pdf_path)\n",
    "            chunks_iter = list(iter_pdf_chunks(pdf_path, chunk_tokens, overlap_tokens))\n",
    "            if not chunks_iter:\n",
    "                print(f\"-- Skipping (no extractable text): {rel_name}\")\n",
    "                continue\n",
    "\n",
    "            # Embed in batches for this file\n",
    "            for i in range(0, len(chunks_iter), EMBED_BATCH):\n",
    "                batch = chunks_iter[i:i + EMBED_BATCH]\n",
    "                texts = [t for (_, _, _, t) in batch]\n",
    "                vecs = embed_texts_openai(oai, embed_model, texts)\n",
    "                for (path_, page, ci, txt), v in zip(batch, vecs):\n",
    "                    total_chunks += 1\n",
    "                    yield {\n",
    "                        \"id\": f\"{file_id_prefix}#p{page}-{ci}-{uuid.uuid4().hex[:6]}\",\n",
    "                        \"values\": v,\n",
    "                        \"metadata\": {\n",
    "                            \"source\": rel_name.replace(\"\\\\\", \"/\"),\n",
    "                            \"page\": page,\n",
    "                            \"chunk\": ci,\n",
    "                            \"model\": embed_model,\n",
    "                            \"text\": txt[:2000]  # helpful snippet; keep metadata reasonable\n",
    "                        }\n",
    "                    }\n",
    "            nonlocal total_files\n",
    "            total_files += 1\n",
    "\n",
    "    # Stream upserts in UPSERT_BATCH chunks\n",
    "    streamed = 0\n",
    "    for upsert_chunk in chunks(gen_upserts(), UPSERT_BATCH):\n",
    "        index.upsert(vectors=list(upsert_chunk), namespace=namespace)\n",
    "        streamed += len(upsert_chunk)\n",
    "        print(f\"Upserted {streamed} vectors so far...\")\n",
    "\n",
    "    print(f\"✅ Done. Files ingested: {total_files}, chunks embedded & upserted: {total_chunks}, namespace: {namespace}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7efb278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 175 vectors so far...\n",
      "✅ Done. Files ingested: 2, chunks embedded & upserted: 175, namespace: __default__\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "import argparse\n",
    "\n",
    "folder = os.path.abspath(\"/workspaces/gdpr_chat/docs\")\n",
    "index_host = \"https://gdpr-chat-index-bw6rwbq.svc.aped-4627-b74a.pinecone.io\"\n",
    "namespace = \"__default__\"\n",
    "embed_model = \"text-embedding-3-small\"\n",
    "chunk_tokens = 700\n",
    "overlap_tokens = 100\n",
    "\n",
    "ingest_folder(\n",
    "    folder=folder,\n",
    "    index_host=index_host,\n",
    "    namespace=namespace,\n",
    "    embed_model=embed_model,\n",
    "    chunk_tokens=chunk_tokens,\n",
    "    overlap_tokens=overlap_tokens\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "029ce1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.722\n",
      "Source: GDPR_Guidelines.pdf\n",
      "Text: GDPR Guidelines – Plain Text Version 1. What is GDPR? The General Data Protection Regulation (GDPR) is a comprehensive European Union law designed to protect individuals' personal data and privacy. It came into effect on 25 May 2018, replacing outdated data protection rules and harmonizing data privacy regulations across EU member states. 2. To Whom Does GDPR Apply? - Applies to organizations within the EU that process personal data as part of their operations, no matter where the processing takes place. - Also applies to entities outside the EU if they offer goods or services to individuals in the EU or monitor their behavior. 3. Key Principles of GDPR Organizations handling personal data must adhere to these core principles: 1. Lawfulness, fairness & transparency – Data must be processed legally, fairly, and transparently. 2. Purpose limitation – Data must be collected for specified, explicit purposes. 3. Data minimization – Only data essential to the purpose should be processed. 4. Accuracy – Data must be accurate and up to date. 5. Storage limitation – Data should be retained only as long as necessary. 6. Integrity & confidentiality – Appropriate security must be in place. 7. Accountability – Organizations must demonstrate compliance. 4. Lawful Bases for Processing Processing of personal data is permitted only if at least one of the following conditions is met: - Explicit consent from the data subject. - Necessary for a contract with the data subject. - Required to meet legal obligations. - Needed to protect vital interests of individuals. - Pursuing a task in the public interest or official function. - Valid for legitimate interests, unless overridden by the rights of the data subject. 5. Rights of Individuals Under GDPR, individuals have the following rights: 1. Right to be informed 2. Right of access 3. Right to rectification 4. Right to erasure (\"right to be forgotten\") 5. Right to restriction of processing 6. Right to data portability 7. Right to object 8. ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query Pinecone index with a text query\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from openai import OpenAI\n",
    "\n",
    "# 1. Prepare clients\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "index = pc.Index(host=\"https://gdpr-chat-index-bw6rwbq.svc.aped-4627-b74a.pinecone.io\")\n",
    "oai = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# 2. Embed your query\n",
    "query_text = \"Why do we need GDPR\"\n",
    "embed_model = \"text-embedding-3-small\"\n",
    "query_vec = oai.embeddings.create(model=embed_model, input=[query_text]).data[0].embedding\n",
    "\n",
    "# 3. Query Pinecone\n",
    "results = index.query(\n",
    "    vector=query_vec,\n",
    "    top_k=1,\n",
    "    namespace=\"__default__\",\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "# 4. Print results\n",
    "for match in results.matches:\n",
    "    print(f\"Score: {match.score:.3f}\")\n",
    "    print(f\"Source: {match.metadata.get('source')}\")\n",
    "    print(f\"Text: {match.metadata.get('text')}...\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gdpr-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
